# Projeto HPC â€” EstimaÃ§Ã£o de Ï€ via MÃ©todo de Monte Carlo Paralelizado

## ğŸ“‹ VisÃ£o Geral

Este projeto implementa a estimaÃ§Ã£o do nÃºmero Ï€ utilizando o mÃ©todo de Monte Carlo paralelizado com **MPI (Message Passing Interface)**. O objetivo Ã© demonstrar tÃ©cnicas de computaÃ§Ã£o de alto desempenho (HPC) atravÃ©s de um problema clÃ¡ssico que permite excelente escalabilidade em ambientes distribuÃ­dos.

**Problema**: Estimar o valor de Ï€ usando simulaÃ§Ãµes probabilÃ­sticas  
**MÃ©todo**: MÃ©todo de Monte Carlo com paralelizaÃ§Ã£o MPI  
**Paralelismo**: MPI para distribuiÃ§Ã£o entre processos  
**Ambiente**: Santos Dumont (SLURM) + desenvolvimento local

## ğŸ¯ RelevÃ¢ncia

- **Benchmark clÃ¡ssico** para avaliaÃ§Ã£o de desempenho em HPC
- **Caso de estudo** ideal para aprendizado de MPI e SLURM
- **Problema embaraÃ§osamente paralelo** - excelente para escalabilidade
- **Base** para problemas mais complexos de simulaÃ§Ã£o cientÃ­fica

## âš™ï¸ Requisitos

### Ambiente Local
- Python 3.10+
- MPI (OpenMPI ou MPICH)
- numpy
- mpi4py

### Santos Dumont
- SLURM workload manager
- MÃ³dulos: python/3.10, mpi

## ğŸš€ Como Executar

### 1. PreparaÃ§Ã£o do ambiente
```bash
git clone <url-do-repositorio>
cd projeto-hpc
```

### 2. InstalaÃ§Ã£o de dependÃªncias
```bash
bash scripts/build.sh
```

### 3. ExecuÃ§Ã£o local

#### Teste serial
```bash
python src/main.py
```

#### Teste paralelo (4 processos)
```bash
bash scripts/run_local.sh
```

#### Perfilamento completo
```bash
bash scripts/profile.sh
```

### 4. GeraÃ§Ã£o de dados de teste

#### Dataset bÃ¡sico
```bash
python data_sample/gerador_dados.py --tamanho 1000000 --salvar
```

#### Dataset completo
```bash
python data_sample/gerador_dados.py --dataset-completo
```

### 5. ExecuÃ§Ã£o no Santos Dumont
```bash
# Acesso
ssh usuario@login.sd.lncc.br
cd /scratch/$USER/projeto-hpc

# SubmissÃ£o do job
sbatch scripts/job_cpu.slurm

# Monitoramento
squeue -u $USER
tail -f results/monte_carlo_pi_*.out
```

## ğŸ“ Estrutura do Projeto
```
projeto-hpc/
â”œâ”€â”€ README.md
â”œâ”€â”€ SUBMISSAO_SD.md
â”œâ”€â”€ env/
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ monte_carlo.py
â”œâ”€â”€ data_sample/
â”‚   â””â”€â”€ gerador_dados.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ build.sh
â”‚   â”œâ”€â”€ run_local.sh
â”‚   â”œâ”€â”€ job_cpu.slurm
â”‚   â”œâ”€â”€ job_scaling.slurm
â”‚   â””â”€â”€ profile.sh
â”œâ”€â”€ results/
â””â”€â”€ report/
    â””â”€â”€ RELATORIO.pdf
```

## ğŸ”§ Scripts Principais

### `scripts/build.sh`
Instala dependÃªncias Python (numpy, mpi4py).

### `scripts/run_local.sh`
Executa teste local com MPI (4 processos).

### `scripts/job_cpu.slurm`
Job SLURM para execuÃ§Ã£o no Santos Dumont (16 processos).

### `scripts/profile.sh`
Script de perfilamento - testa escalabilidade com 1-16 processos.

## ğŸ“Š Resultados Esperados

### SaÃ­da do programa:
```
PI estimado = 3.1415926535
Tempo = 2.3456 segundos  
Processos = 16
Amostras totais = 10,000,000
```

### MÃ©tricas de desempenho:
- **Speedup**: â‰ˆ14.5Ã— com 16 processos
- **EficiÃªncia**: â‰ˆ91% com 16 processos
- **PrecisÃ£o**: â‰ˆ10â»â¶ de erro absoluto

## âš ï¸ Troubleshooting

### Problemas comuns:
```bash
# MPI nÃ£o encontrado (Linux)
sudo apt install mpich libopenmpi-dev

# DependÃªncias missing
pip install numpy mpi4py

# Erro de permissÃ£o
chmod +x scripts/*.sh
```

### No Windows:
```bash
# Instale MS-MPI e use
mpiexec -n 4 python src/main.py
```

### No Santos Dumont:
- Use `/scratch` para dados temporÃ¡rios
- NÃ£o execute cargas pesadas no nÃ³ de login
- Monitore jobs com `squeue` e `sacct`

## ğŸ“ˆ AnÃ¡lise de Desempenho

O projeto inclui ferramentas para anÃ¡lise completa:
- Perfilamento automÃ¡tico com diferentes nÃºmeros de processos
- GeraÃ§Ã£o de grÃ¡ficos de speedup e tempo de execuÃ§Ã£o
- MÃ©tricas de eficiÃªncia e overhead de comunicaÃ§Ã£o

## ğŸ‘¥ Autores

- Matheus Ferrarezi Mesquita Fagundes 
- Disciplina: Sistemas Distribuidos 
- InstituiÃ§Ã£o: Uni-FACEF 
- Data: 2025-09-24

